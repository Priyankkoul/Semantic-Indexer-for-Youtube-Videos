{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTSMftRnoEMT",
        "outputId": "0bcaa2ce-dee2-48ca-f5ff-8b5381739876"
      },
      "source": [
        "import re\n",
        "from collections             import defaultdict\n",
        "import gensim\n",
        "from gensim.utils            import tokenize\n",
        "from youtube_transcript_api  import YouTubeTranscriptApi\n",
        "from nltk.corpus             import brown\n",
        "import nltk\n",
        "nltk.download('brown')\n",
        "languages = ['en']\n",
        "youtube = r'(youtu.be\\/|v\\/|e\\/|u\\/\\w+\\/|embed\\/|v=)'\n",
        "video_id = r'([^#\\&\\?]*).*'\n",
        "https = r'^.*'\n",
        "query=\"let us take an example\"\n",
        "parsed_url = re.search(https + youtube + video_id, \"https://www.youtube.com/watch?v=J7DzL2_Na80\")\n",
        "video_id = parsed_url[2]\n",
        "captions_and_timestamps = dict()\n",
        "try:\n",
        "    captions_and_timestamps = YouTubeTranscriptApi.get_transcript(video_id, languages)\n",
        "except Exception as _e_:\n",
        "    print(_e_, type(_e_))\n",
        "captions = defaultdict(float)\n",
        "for data in captions_and_timestamps:\n",
        "    captions[data['text'].lower()] = data['start']\n",
        "captions_and_timestamps = captions\n",
        "def compute_semantic_similarity(sentence_1, sentence_2):\n",
        "    \"\"\"\n",
        "    uses word2vec\n",
        "    modified version of the WMD between sentences/documents of varying lengths.\n",
        "    \"\"\"\n",
        "\n",
        "    sentence_1 = list(tokenize(sentence_1))\n",
        "    sentence_2 = list(tokenize(sentence_2))\n",
        "\n",
        "    visited_1 = [0]*len(sentence_1)\n",
        "    visited_2 = [0]*len(sentence_2)\n",
        "\n",
        "    similarity = 0\n",
        "\n",
        "    # The algorithm used below is word-mover's distance algorithm.\n",
        "    # It is asymmetric, and for each word in sentence_1, we find the closest\n",
        "    # un-mapped word in sentence_2 and add this similarity.\n",
        "    for idx_a, word_a in enumerate(sentence_1):\n",
        "        if vocabulary[word_a] == 1:\n",
        "            visited_1[idx_a] = 1\n",
        "            closest_distance = 1e18\n",
        "            idx_chosen = -1\n",
        "            for idx_b, word_b in enumerate(sentence_2):\n",
        "                if visited_2[idx_b] == 0 and vocabulary[word_b] == 1:\n",
        "                    current_distance = (1 - model.similarity(word_a, word_b))\n",
        "                    if idx_chosen == -1 or current_distance < closest_distance:\n",
        "                        closest_distance = min(closest_distance, current_distance)\n",
        "                        idx_chosen = idx_b\n",
        "\n",
        "            if idx_chosen != -1:\n",
        "                visited_2[idx_chosen] = 1\n",
        "\n",
        "            similarity += closest_distance\n",
        "\n",
        "    return similarity/len(sentence_1)\n",
        "def pretty_print(output):\n",
        "    spaces = max([len(caption) for caption, timestamp in output])\n",
        "    spaces += 5\n",
        "    for caption, timestamp in output:\n",
        "        print(caption, end=' '*(spaces - len(caption)))\n",
        "        print(timestamp)\n",
        "\n",
        "def normalize_time(timestamps):\n",
        "    normalized_time = []\n",
        "\n",
        "    for timestamp in timestamps:\n",
        "        seconds = int(timestamp)\n",
        "        minutes = seconds//60\n",
        "        hours = minutes//60\n",
        "        minutes = minutes%60\n",
        "        seconds = seconds%60\n",
        "\n",
        "        normalized_timestamp = ''\n",
        "\n",
        "        if hours != 0:\n",
        "            normalized_timestamp += str(hours) + 'h '\n",
        "        normalized_timestamp += str(minutes)+'m '+str(seconds)+'s'\n",
        "\n",
        "        normalized_time.append(normalized_timestamp)\n",
        "\n",
        "    return normalized_time\n",
        "query = query.lower()\n",
        "sentences = brown.sents()\n",
        "model = gensim.models.Word2Vec(sentences, min_count=1)\n",
        "vocabulary = defaultdict(int)\n",
        "for sentence in sentences:\n",
        "    for word in sentence:\n",
        "        vocabulary[word] = 1\n",
        "captions_and_similarities = []\n",
        "corpus = list(captions_and_timestamps.keys())\n",
        "limit=10\n",
        "for idx, caption in enumerate(corpus):\n",
        "    similarity = compute_semantic_similarity(query, caption)\n",
        "    captions_and_similarities.append([similarity, idx])\n",
        "captions_and_similarities.sort()\n",
        "captions_and_similarities = captions_and_similarities[:limit]\n",
        "most_similar_captions = []\n",
        "for similarity, idx in captions_and_similarities:\n",
        "    most_similar_captions.append(corpus[idx])\n",
        "timestamps = [captions_and_timestamps[caption] for caption in most_similar_captions]\n",
        "timestamps_and_captions = defaultdict(str)\n",
        "for caption in captions_and_timestamps:\n",
        "    key = captions_and_timestamps[caption]\n",
        "    value = caption\n",
        "    timestamps_and_captions[key] = value\n",
        "captions_extracted = []\n",
        "for timestamp in timestamps:\n",
        "    captions_extracted.append(timestamps_and_captions[timestamp])\n",
        "pretty_print(list(zip(captions_extracted, normalize_time(timestamps))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:49: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "let me do a three\n",
            "by three example.                               15m 38s\n",
            "oh, let me take a\n",
            "different right-hand side.                      26m 24s\n",
            "and let me take a vector\n",
            "x to be, say, 1and 2.                    36m 21s\n",
            "let me make that a minus one\n",
            "and, just for variety let me         15m 55s\n",
            "for now, let's stay with a nice\n",
            "case where the matrices work,     35m 18s\n",
            "okay, so let me take\n",
            "the column picture.                          22m 4s\n",
            "so let's start\n",
            "with a case when we                                1m 13s\n",
            "so let me come back next\n",
            "time to a systematic way,                39m 12s\n",
            "shall we take just a\n",
            "little shot at thinking                      32m 27s\n",
            "by the big picture i mean\n",
            "let's keep this same matrix             26m 15s\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}